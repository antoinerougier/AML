{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, log_loss, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"data/voice.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "data['label'] = data['label'].map({'male': 1, 'female': 0})\n",
    "\n",
    "X = data.drop(columns=['label']).values\n",
    "y = data['label'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train = (X_train - X_train.mean(axis=0)) / X_train.std(axis=0)\n",
    "X_test = (X_test - X_train.mean(axis=0)) / X_train.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation functions\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_safe(z):\n",
    "    return np.clip(sigmoid(z), 1e-7, 1 - 1e-7)\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def relu_derivative(z):\n",
    "    return np.where(z > 0, 1, 0)\n",
    "\n",
    "# Batch normalization with noise\n",
    "def batch_norm_forward(Z, gamma, beta, noise_scale=0.1, eps=1e-5):\n",
    "    mu = np.mean(Z, axis=1, keepdims=True)\n",
    "    var = np.var(Z, axis=1, keepdims=True)\n",
    "    Z_norm = (Z - mu) / np.sqrt(var + eps)\n",
    "    noisy_Z_norm = Z_norm + np.random.normal(0, noise_scale, Z_norm.shape)  # Add noise\n",
    "    out = gamma * noisy_Z_norm + beta\n",
    "    cache = (Z, Z_norm, mu, var, gamma, beta, eps)\n",
    "    return out, cache\n",
    "\n",
    "def batch_norm_backward(dout, cache):\n",
    "    Z, Z_norm, mu, var, gamma, beta, eps = cache\n",
    "    m = Z.shape[1]\n",
    "\n",
    "    dZ_norm = dout * gamma\n",
    "    dvar = np.sum(dZ_norm * (Z - mu) * -0.5 * (var + eps)**(-1.5), axis=1, keepdims=True)\n",
    "    dmu = np.sum(dZ_norm * -1 / np.sqrt(var + eps), axis=1, keepdims=True) + dvar * np.sum(-2 * (Z - mu), axis=1, keepdims=True) / m\n",
    "\n",
    "    dZ = dZ_norm / np.sqrt(var + eps) + dvar * 2 * (Z - mu) / m + dmu / m\n",
    "    dgamma = np.sum(dout * Z_norm, axis=1, keepdims=True)\n",
    "    dbeta = np.sum(dout, axis=1, keepdims=True)\n",
    "\n",
    "    return dZ, dgamma, dbeta\n",
    "\n",
    "# Initialization\n",
    "def initialize_parameters(layers):\n",
    "    parameters = {}\n",
    "    np.random.seed(42)\n",
    "\n",
    "    for l in range(1, len(layers)):\n",
    "        parameters[f'W{l}'] = np.random.randn(layers[l], layers[l-1]) * 0.01\n",
    "        parameters[f'b{l}'] = np.zeros((layers[l], 1))\n",
    "        parameters[f'gamma{l}'] = np.ones((layers[l], 1))\n",
    "        parameters[f'beta{l}'] = np.zeros((layers[l], 1))\n",
    "\n",
    "    return parameters\n",
    "\n",
    "# Forward propagation\n",
    "def forward_propagation(X, parameters, layers, use_batch_norm=True, noise_scale=0.1):\n",
    "    caches = []\n",
    "    A = X\n",
    "\n",
    "    for l in range(1, len(layers)):\n",
    "        Z = parameters[f'W{l}'].dot(A) + parameters[f'b{l}']\n",
    "        if use_batch_norm:\n",
    "            A, cache_bn = batch_norm_forward(Z, parameters[f'gamma{l}'], parameters[f'beta{l}'], noise_scale)\n",
    "        else:\n",
    "            A = Z\n",
    "        A = relu(A) if l < len(layers) - 1 else sigmoid_safe(A)\n",
    "        caches.append((A, cache_bn) if use_batch_norm else (A, None))\n",
    "\n",
    "    return A, caches\n",
    "\n",
    "# Backward propagation\n",
    "def back_propagation(X, y, parameters, caches, layers, use_batch_norm=True):\n",
    "    gradients = {}\n",
    "    m = y.shape[1]\n",
    "    A_prev, _ = caches[-1]\n",
    "    dA = A_prev - y\n",
    "\n",
    "    for l in reversed(range(1, len(layers))):\n",
    "        A_prev, cache_bn = caches[l-1]\n",
    "        dA_relu = dA * (relu_derivative(A_prev) if l < len(layers) - 1 else 1)\n",
    "\n",
    "        if use_batch_norm:\n",
    "            dZ, dgamma, dbeta = batch_norm_backward(dA_relu, cache_bn)\n",
    "            gradients[f'dgamma{l}'] = dgamma\n",
    "            gradients[f'dbeta{l}'] = dbeta\n",
    "        else:\n",
    "            dZ = dA_relu\n",
    "\n",
    "        gradients[f'dW{l}'] = 1 / m * dZ.dot(caches[l-2][0].T if l > 1 else X.T)\n",
    "        gradients[f'db{l}'] = 1 / m * np.sum(dZ, axis=1, keepdims=True)\n",
    "\n",
    "        if l > 1:\n",
    "            dA = parameters[f'W{l}'].T.dot(dZ)\n",
    "\n",
    "    return gradients\n",
    "\n",
    "# Update parameters\n",
    "def update_parameters(parameters, gradients, learning_rate):\n",
    "    for key in parameters.keys():\n",
    "        if key in gradients:\n",
    "            parameters[key] -= learning_rate * gradients[f'd{key}']\n",
    "\n",
    "    return parameters\n",
    "\n",
    "# Prediction\n",
    "def predict(X, parameters, layers, use_batch_norm=True, noise_scale=0.1):\n",
    "    A, _ = forward_propagation(X, parameters, layers, use_batch_norm, noise_scale)\n",
    "    return (A >= 0.5).astype(int)\n",
    "\n",
    "# Mini-batch generator\n",
    "def mini_batch_generator(X, y, batch_size):\n",
    "    m = X.shape[1]\n",
    "    permutation = np.random.permutation(m)\n",
    "    X_shuffled = X[:, permutation]\n",
    "    y_shuffled = y[:, permutation]\n",
    "\n",
    "    for i in range(0, m, batch_size):\n",
    "        X_batch = X_shuffled[:, i:i+batch_size]\n",
    "        y_batch = y_shuffled[:, i:i+batch_size]\n",
    "        yield X_batch, y_batch\n",
    "\n",
    "# Neural network training\n",
    "def neural_network(X, y, layers, learning_rate=0.1, epochs=10, batch_size=32, use_batch_norm=True, noise_scale=0.1):\n",
    "    parameters = initialize_parameters(layers)\n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = []\n",
    "        epoch_acc = []\n",
    "\n",
    "        for X_batch, y_batch in mini_batch_generator(X, y, batch_size):\n",
    "            A, caches = forward_propagation(X_batch, parameters, layers, use_batch_norm, noise_scale)\n",
    "\n",
    "            epoch_loss.append(log_loss(y_batch.flatten(), A.flatten()))\n",
    "            y_pred = (A >= 0.5).astype(int)\n",
    "            epoch_acc.append(accuracy_score(y_batch.flatten(), y_pred.flatten()))\n",
    "\n",
    "            gradients = back_propagation(X_batch, y_batch, parameters, caches, layers, use_batch_norm)\n",
    "            parameters = update_parameters(parameters, gradients, learning_rate)\n",
    "\n",
    "        train_loss.append(np.mean(epoch_loss))\n",
    "        train_acc.append(np.mean(epoch_acc))\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} (BatchNorm={'On' if use_batch_norm else 'Off'}): Loss = {train_loss[-1]:.4f}, Accuracy = {train_acc[-1]:.4f}\")\n",
    "\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_loss, label='Train Loss')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_acc, label='Train Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation\n",
    "X = X_train.T\n",
    "y = y_train.reshape((1, y_train.shape[0]))\n",
    "\n",
    "layers = [X.shape[0], 16, 8, 4, 1]  # Example with 3 hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network with BatchNorm and noise\n",
    "parameters_bn_noise = neural_network(X, y, layers, learning_rate=0.1, epochs=30, batch_size=64, use_batch_norm=True, noise_scale=0.5)\n",
    "\n",
    "# Neural network with BatchNorm (no noise)\n",
    "parameters_bn = neural_network(X, y, layers, learning_rate=0.1, epochs=30, batch_size=64, use_batch_norm=True, noise_scale=0.0)\n",
    "\n",
    "# Neural network without BatchNorm\n",
    "parameters_no_bn = neural_network(X, y, layers, learning_rate=0.1, epochs=30, batch_size=64, use_batch_norm=False)\n",
    "\n",
    "X_test_ = X_test.T\n",
    "y_test_ = y_test.reshape((1, y_test.shape[0]))\n",
    "\n",
    "# Evaluation\n",
    "configs = [\n",
    "    (\"BatchNorm + Noise\", parameters_bn_noise, True, 0.1),\n",
    "    (\"BatchNorm\", parameters_bn, True, 0.0),\n",
    "    (\"No BatchNorm\", parameters_no_bn, False, 0.0),\n",
    "]\n",
    "\n",
    "for config_name, params, use_bn, noise_scale in configs:\n",
    "    y_test_pred = predict(X_test_, params, layers, use_batch_norm=use_bn, noise_scale=noise_scale)\n",
    "\n",
    "    accuracy = accuracy_score(y_test_.flatten(), y_test_pred.flatten())\n",
    "    precision = precision_score(y_test_.flatten(), y_test_pred.flatten())\n",
    "    recall = recall_score(y_test_.flatten(), y_test_pred.flatten())\n",
    "    f1 = f1_score(y_test_.flatten(), y_test_pred.flatten())\n",
    "\n",
    "    print(f\"Performance ({config_name}):\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "    cm = confusion_matrix(y_test.flatten(), y_test_pred.flatten())\n",
    "    cm_display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Female', 'Male'])\n",
    "    cm_display.plot(cmap='Blues')\n",
    "    plt.title(f\"Confusion Matrix ({config_name})\")\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
