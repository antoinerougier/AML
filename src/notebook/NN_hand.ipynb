{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data upload \n",
    "file_path = \"data/voice.csv\"  \n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "data['label'] = data['label'].map({'male': 1, 'female': 0})\n",
    "\n",
    "X = data.drop(columns=['label']).values  \n",
    "y = data['label'].values  \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "X = X_train.T\n",
    "y = y_train.reshape((1, y_train.shape[0]))\n",
    "\n",
    "X_test = X_test.T\n",
    "y_test = y_test.reshape((1, y_test.shape[0]))\n",
    "y_test = y_test.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_safe(z):\n",
    "    return np.clip(sigmoid(z), 1e-7, 1 - 1e-7)\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def relu_derivative(z):\n",
    "    return np.where(z > 0, 1, 0)\n",
    "\n",
    "def binary_cross_entropy(y_true, y_pred):\n",
    "    return -np.mean(y_true * np.log(y_pred + 1e-8) + (1 - y_true) * np.log(1 - y_pred + 1e-8))\n",
    "\n",
    "def binary_cross_entropy_derivative(y_true, y_pred):\n",
    "    return (y_pred - y_true) / (y_pred * (1 - y_pred) + 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first neural network\n",
    "def initialisation(n0, n1, n2):\n",
    "    W1 = np.random.randn(n1, n0) * 0.01\n",
    "    b1 = np.zeros((n1, 1))\n",
    "    W2 = np.random.randn(n2, n1) * 0.01\n",
    "    b2 = np.zeros((n2, 1))\n",
    "    parameter = {'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n",
    "    return parameter\n",
    "\n",
    "def forward_propagation(X, parameter):\n",
    "    W1 = parameter['W1']\n",
    "    W2 = parameter['W2']\n",
    "    b1 = parameter['b1']\n",
    "    b2 = parameter['b2']\n",
    "    Z1 = W1.dot(X) + b1\n",
    "    A1 = relu(Z1)\n",
    "    Z2 = W2.dot(A1) + b2\n",
    "    A2 = sigmoid_safe(Z2)\n",
    "    activations = {'A1': A1, 'A2': A2}\n",
    "    return activations\n",
    "\n",
    "def back_propragation(X, y, parameter, activations):\n",
    "    A1 = activations['A1']\n",
    "    A2 = activations['A2']\n",
    "    W2 = parameter['W2']\n",
    "    m = y.shape[1]\n",
    "    dZ2 = A2 - y\n",
    "    dW2 = 1 / m * dZ2.dot(A1.T)\n",
    "    db2 = 1 / m * np.sum(dZ2, axis=1, keepdims=True)\n",
    "    dZ1 = np.dot(W2.T, dZ2) * relu_derivative(A1)\n",
    "    dW1 = 1 / m * dZ1.dot(X.T)\n",
    "    db1 = 1 / m * np.sum(dZ1, axis=1, keepdims=True)\n",
    "    gradients = {'db1': db1, 'db2': db2, 'dW1': dW1, 'dW2': dW2}\n",
    "    return gradients\n",
    "\n",
    "def update(gradients, parameter, learning_rate):\n",
    "    dW1 = gradients['dW1']\n",
    "    dW2 = gradients['dW2']\n",
    "    db1 = gradients['db1']\n",
    "    db2 = gradients['db2']\n",
    "    W1 = parameter['W1']\n",
    "    W2 = parameter['W2']\n",
    "    b1 = parameter['b1']\n",
    "    b2 = parameter['b2']\n",
    "    W1 = W1 - learning_rate * dW1\n",
    "    W2 = W2 - learning_rate * dW2\n",
    "    b1 = b1 - learning_rate * db1\n",
    "    b2 = b2 - learning_rate * db2\n",
    "    parameter = {'W1': W1, 'W2': W2, 'b1': b1, 'b2': b2}\n",
    "    return parameter\n",
    "\n",
    "def predict(X, parameter):\n",
    "    activations = forward_propagation(X, parameter)\n",
    "    A2 = activations['A2']\n",
    "    return (A2 >= 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_network(X, y, n1=16, learning_rate=0.1, epochs=50, batch_size=32):\n",
    "    n0 = X.shape[0]\n",
    "    n2 = y.shape[0]\n",
    "    np.random.seed(0)\n",
    "    parametres = initialisation(n0, n1, n2)\n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "    \n",
    "    # Example numbers\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = []\n",
    "        epoch_acc = []\n",
    "        \n",
    "        # Mini-batch\n",
    "        permutation = np.random.permutation(m)\n",
    "        X_shuffled = X[:, permutation]\n",
    "        y_shuffled = y[:, permutation]\n",
    "        \n",
    "        # Mini-batch training\n",
    "        for i in range(0, m, batch_size):\n",
    "            X_batch = X_shuffled[:, i:i+batch_size]\n",
    "            y_batch = y_shuffled[:, i:i+batch_size]\n",
    "            \n",
    "            # Forward pass\n",
    "            activations = forward_propagation(X_batch, parametres)\n",
    "            A2 = activations['A2']\n",
    "            \n",
    "            # Calculation of the loss\n",
    "            loss = log_loss(y_batch.flatten(), A2.flatten())\n",
    "            y_pred = predict(X_batch, parametres)\n",
    "            acc = accuracy_score(y_batch.flatten(), y_pred.flatten())\n",
    "            \n",
    "            epoch_loss.append(loss)\n",
    "            epoch_acc.append(acc)\n",
    "            \n",
    "            # Backward pass\n",
    "            gradients = back_propragation(X_batch, y_batch, parametres, activations)\n",
    "            \n",
    "            # Update of the parameters\n",
    "            parametres = update(gradients, parametres, learning_rate)\n",
    "        \n",
    "        # Calculation of the loss and the metrics accuracy\n",
    "        train_loss.append(np.mean(epoch_loss))\n",
    "        train_acc.append(np.mean(epoch_acc))\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {train_loss[-1]:.4f}, Accuracy: {train_acc[-1]:.4f}\")\n",
    "    \n",
    "    # Plot of the train set\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_loss, label='Train Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_acc, label='Train Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return parametres\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parametres_final = neural_network(X, y, n1=16, learning_rate=0.01, epochs=300, batch_size=64)\n",
    "\n",
    "y_test_pred = predict(X_test, parametres_final)  \n",
    "\n",
    "accuracy = accuracy_score(y_test, y_test_pred.flatten())\n",
    "precision = precision_score(y_test, y_test_pred.flatten())\n",
    "recall = recall_score(y_test, y_test_pred.flatten())\n",
    "f1 = f1_score(y_test, y_test_pred.flatten())\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "cm = confusion_matrix(y_test, y_test_pred.flatten())\n",
    "\n",
    "cm_display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Female', 'Male'])\n",
    "cm_display.plot(cmap='Blues')\n",
    "plt.title(\"Matrice de confusion\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update of fonction predict and forward \n",
    "def forward_propagation_norm(X, parameter):\n",
    "    W1 = parameter['W1']\n",
    "    W2 = parameter['W2']\n",
    "    b1 = parameter['b1']\n",
    "    b2 = parameter['b2']\n",
    "    Z1 = W1.dot(X) + b1\n",
    "    \n",
    "    # Normalisation before the activation function\n",
    "    mean_Z1 = np.mean(Z1, axis=1, keepdims=True)\n",
    "    std_Z1 = np.std(Z1, axis=1, keepdims=True)\n",
    "    Z1_normalized = (Z1 - mean_Z1) / (std_Z1 + 1e-8)  # we add a little epsilon for avoiding to divide by 0 \n",
    "    \n",
    "    A1 = relu(Z1_normalized)\n",
    "    \n",
    "    Z2 = W2.dot(A1) + b2\n",
    "    A2 = sigmoid_safe(Z2)\n",
    "    \n",
    "    activations = {'A1': A1, 'A2': A2}\n",
    "    return activations\n",
    "\n",
    "def predict_norm(X, parameter):\n",
    "    activations = forward_propagation_norm(X, parameter)\n",
    "    A2 = activations['A2']\n",
    "    return (A2 >= 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_network_norm(X, y, n1=16, learning_rate=0.1, epochs=50, batch_size=32):\n",
    "    n0 = X.shape[0]\n",
    "    n2 = y.shape[0]\n",
    "    np.random.seed(0)\n",
    "    parametres = initialisation(n0, n1, n2)\n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = []\n",
    "        epoch_acc = []\n",
    "        \n",
    "        permutation = np.random.permutation(m)\n",
    "        X_shuffled = X[:, permutation]\n",
    "        y_shuffled = y[:, permutation]\n",
    "        \n",
    "        # Mini-batch training\n",
    "        for i in range(0, m, batch_size):\n",
    "            X_batch = X_shuffled[:, i:i+batch_size]\n",
    "            y_batch = y_shuffled[:, i:i+batch_size]\n",
    "            \n",
    "            # Forward pass\n",
    "            activations = forward_propagation_norm(X_batch, parametres)\n",
    "            A2 = activations['A2']\n",
    "            \n",
    "            loss = log_loss(y_batch.flatten(), A2.flatten())\n",
    "            y_pred = predict_norm(X_batch, parametres)\n",
    "            acc = accuracy_score(y_batch.flatten(), y_pred.flatten())\n",
    "            \n",
    "            epoch_loss.append(loss)\n",
    "            epoch_acc.append(acc)\n",
    "            \n",
    "            # Backward pass\n",
    "            gradients = back_propragation(X_batch, y_batch, parametres, activations)\n",
    "            \n",
    "            parametres = update(gradients, parametres, learning_rate)\n",
    "        \n",
    "        train_loss.append(np.mean(epoch_loss))\n",
    "        train_acc.append(np.mean(epoch_acc))\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {train_loss[-1]:.4f}, Accuracy: {train_acc[-1]:.4f}\")\n",
    "    \n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_loss, label='Train Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_acc, label='Train Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return parametres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parametres_final_norm = neural_network_norm(X, y, n1=16, learning_rate=0.01, epochs=300, batch_size=64)\n",
    "\n",
    "y_test_pred = predict_norm(X_test, parametres_final_norm)  \n",
    "\n",
    "accuracy = accuracy_score(y_test, y_test_pred.flatten())\n",
    "precision = precision_score(y_test, y_test_pred.flatten())\n",
    "recall = recall_score(y_test, y_test_pred.flatten())\n",
    "f1 = f1_score(y_test, y_test_pred.flatten())\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "cm = confusion_matrix(y_test, y_test_pred.flatten())\n",
    "\n",
    "cm_display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Female', 'Male'])\n",
    "cm_display.plot(cmap='Blues')\n",
    "plt.title(\"Matrice de confusion\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_normalization(Z, gamma, beta, epsilon=1e-8):\n",
    "    mean = np.mean(Z, axis=1, keepdims=True)\n",
    "    variance = np.var(Z, axis=1, keepdims=True)\n",
    "    \n",
    "    # Normalization\n",
    "    Z_norm = (Z - mean) / np.sqrt(variance + epsilon)\n",
    "    \n",
    "    # Linear transformation\n",
    "    Z_out = gamma * Z_norm + beta\n",
    "    \n",
    "    return Z_out, mean, variance, Z_norm\n",
    "\n",
    "def forward_propagation_BN(X, parameter, gamma1, beta1):\n",
    "    W1 = parameter['W1']\n",
    "    W2 = parameter['W2']\n",
    "    b1 = parameter['b1']\n",
    "    b2 = parameter['b2']\n",
    "    Z1 = W1.dot(X) + b1\n",
    "    \n",
    "    # Batch Normalization of the hidden layer\n",
    "    Z1_bn, mean_Z1, var_Z1, Z1_norm = batch_normalization(Z1, gamma1, beta1)\n",
    "    \n",
    "    # Activation function after the BN\n",
    "    A1 = relu(Z1_bn)\n",
    "    \n",
    "    Z2 = W2.dot(A1) + b2\n",
    "    A2 = sigmoid_safe(Z2)\n",
    "    \n",
    "    activations = {'A1': A1, 'A2': A2}\n",
    "    cache = {'mean_Z1': mean_Z1, 'var_Z1': var_Z1, 'Z1_norm': Z1_norm}\n",
    "    \n",
    "    return activations, cache\n",
    "\n",
    "def back_propagation_BN(X, y, parameter, activations, cache, gamma1, beta1):\n",
    "    A1 = activations['A1']\n",
    "    A2 = activations['A2']\n",
    "    W2 = parameter['W2']\n",
    "    m = y.shape[1]\n",
    "    \n",
    "    # back to the 2 hidden layers\n",
    "    dZ2 = A2 - y\n",
    "    dW2 = 1 / m * dZ2.dot(A1.T)\n",
    "    db2 = 1 / m * np.sum(dZ2, axis=1, keepdims=True)\n",
    "    \n",
    "    # Backpropagation to the 1 hidden layers\n",
    "    dA1 = W2.T.dot(dZ2)\n",
    "    dZ1_bn = dA1 * relu_derivative(A1)\n",
    "    \n",
    "    Z1_norm = cache['Z1_norm']\n",
    "    mean_Z1 = cache['mean_Z1']\n",
    "    var_Z1 = cache['var_Z1']\n",
    "    \n",
    "    m = Z1_norm.shape[1]\n",
    "    \n",
    "    # calculation of the gradients\n",
    "    dZ1 = (1. / m) * gamma1 * (1. / np.sqrt(var_Z1 + 1e-8)) * (\n",
    "        m * dZ1_bn - np.sum(dZ1_bn, axis=1, keepdims=True) - Z1_norm * np.sum(dZ1_bn * Z1_norm, axis=1, keepdims=True) / (var_Z1 + 1e-8)\n",
    "    )\n",
    "    \n",
    "    dW1 = 1 / m * dZ1.dot(X.T)\n",
    "    db1 = 1 / m * np.sum(dZ1, axis=1, keepdims=True)\n",
    "    \n",
    "    dgamma1 = np.sum(dZ1_bn * Z1_norm, axis=1, keepdims=True)\n",
    "    dbeta1 = np.sum(dZ1_bn, axis=1, keepdims=True)\n",
    "    \n",
    "    gradients = {'dW1': dW1, 'db1': db1, 'dW2': dW2, 'db2': db2, 'dgamma1': dgamma1, 'dbeta1': dbeta1}\n",
    "    \n",
    "    return gradients\n",
    "\n",
    "def update_BN(gradients, parameter, gamma1, beta1, learning_rate):\n",
    "    dW1 = gradients['dW1']\n",
    "    dW2 = gradients['dW2']\n",
    "    db1 = gradients['db1']\n",
    "    db2 = gradients['db2']\n",
    "    W1 = parameter['W1']\n",
    "    W2 = parameter['W2']\n",
    "    b1 = parameter['b1']\n",
    "    b2 = parameter['b2']\n",
    "    \n",
    "    # Update of parameters of the network\n",
    "    W1 = W1 - learning_rate * dW1\n",
    "    W2 = W2 - learning_rate * dW2\n",
    "    b1 = b1 - learning_rate * db1\n",
    "    b2 = b2 - learning_rate * db2\n",
    "    \n",
    "    # Update of the parameters of the BN\n",
    "    gamma1 = gamma1 - learning_rate * gradients['dgamma1']\n",
    "    beta1 = beta1 - learning_rate * gradients['dbeta1']\n",
    "    \n",
    "    parameter = {'W1': W1, 'W2': W2, 'b1': b1, 'b2': b2}\n",
    "    \n",
    "    return parameter, gamma1, beta1\n",
    "\n",
    "def predict_BN(X, parameter):\n",
    "    activations, _ = forward_propagation_BN(X, parameter, gamma1, beta1)\n",
    "    A2 = activations['A2']\n",
    "    return (A2 >= 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_network_BN(X, y, n1=16, learning_rate=0.1, epochs=50, batch_size=32):\n",
    "    n0 = X.shape[0]\n",
    "    n2 = y.shape[0]\n",
    "    \n",
    "    np.random.seed(0)\n",
    "    parametres = initialisation(n0, n1, n2)\n",
    "    \n",
    "    # Initialization of the parameters\n",
    "    gamma1 = np.ones((n1, 1))\n",
    "    beta1 = np.zeros((n1, 1))\n",
    "    \n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = []\n",
    "        epoch_acc = []\n",
    "        \n",
    "        permutation = np.random.permutation(m)\n",
    "        X_shuffled = X[:, permutation]\n",
    "        y_shuffled = y[:, permutation]\n",
    "        \n",
    "        # Mini-batch training\n",
    "        for i in range(0, m, batch_size):\n",
    "            X_batch = X_shuffled[:, i:i+batch_size]\n",
    "            y_batch = y_shuffled[:, i:i+batch_size]\n",
    "            \n",
    "            # Forward pass\n",
    "            activations, cache = forward_propagation_BN(X_batch, parametres, gamma1, beta1)\n",
    "            A2 = activations['A2']\n",
    "            \n",
    "            loss = log_loss(y_batch.flatten(), A2.flatten())\n",
    "            y_pred = predict_BN(X_batch, parametres)\n",
    "            acc = accuracy_score(y_batch.flatten(), y_pred.flatten())\n",
    "            \n",
    "            epoch_loss.append(loss)\n",
    "            epoch_acc.append(acc)\n",
    "            \n",
    "            gradients = back_propagation_BN(X_batch, y_batch, parametres, activations, cache, gamma1, beta1)\n",
    "            \n",
    "            parametres, gamma1, beta1 = update_BN(gradients, parametres, gamma1, beta1, learning_rate)\n",
    "        \n",
    "        train_loss.append(np.mean(epoch_loss))\n",
    "        train_acc.append(np.mean(epoch_acc))\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {train_loss[-1]:.4f}, Accuracy: {train_acc[-1]:.4f}\")\n",
    "    \n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_loss, label='Train Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_acc, label='Train Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return parametres\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma1 = np.ones((16, 1))\n",
    "beta1 = np.zeros((16, 1))\n",
    "\n",
    "parametres_final = neural_network_BN(X, y, n1=16, learning_rate=0.01, epochs=300, batch_size=64)\n",
    "\n",
    "y_test_pred = predict_BN(X_test, parametres_final)  \n",
    "\n",
    "accuracy = accuracy_score(y_test, y_test_pred.flatten())\n",
    "precision = precision_score(y_test, y_test_pred.flatten())\n",
    "recall = recall_score(y_test, y_test_pred.flatten())\n",
    "f1 = f1_score(y_test, y_test_pred.flatten())\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "cm = confusion_matrix(y_test, y_test_pred.flatten())\n",
    "\n",
    "cm_display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Female', 'Male'])\n",
    "cm_display.plot(cmap='Blues')\n",
    "plt.title(\"Matrice de confusion\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
